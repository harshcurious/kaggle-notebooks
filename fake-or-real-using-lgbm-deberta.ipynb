{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":105874,"databundleVersionId":12964783,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fake or Real: End-to-End Text Classification\n\n## 1. Project Overview\n\nThis project aims to solve the binary classification task of “real vs. fake text recognition.” Each sample contains two texts (`text1` and `text2`), one of which is genuine and the other fabricated. Our goal is to build a highly accurate model that predicts which text is real.\n\nThis script integrates the strengths of multiple solutions to construct a robust, efficient, and high-precision end-to-end pipeline. Its main features include:\n\n* **Data Augmentation**: Expanding training data by swapping text pairs and reversing labels, thereby enhancing the model’s generalization ability.\n* **Dual-Pillar Hybrid Modeling**:\n\n  * **The Hammer of Feature Engineering (LightGBM)**: Builds a multi-level, high-dimensional feature set covering stylometry, readability, semantic similarity, text complexity (perplexity), and more, then trains with LightGBM.\n  * **The Blade of Deep Learning (Siamese DeBERTa)**: Employs a conceptual Siamese Network with `DeBERTa-v3` as the backbone, focusing on capturing deep semantic differences between text pairs.\n* **Multi-Level Model Ensemble**:\n\n  * **Level 1 (Blending)**: Weighted fusion of predictions from LightGBM and DeBERTa.\n  * **Level 2 (Stacking)**: Uses Level 1 predictions as meta-features to train a meta-model for the final prediction.\n* **Cascaded Inference**: To optimize prediction speed, the faster LightGBM model first handles “easy” high-confidence samples, while “hard” samples are passed to the more complex but more accurate ensemble model.\n* **Comprehensive Analysis & Visualization**: Provides visualizations such as feature importance, model performance comparison, prediction probability distribution, and error analysis to gain deeper insights into model behavior.","metadata":{}},{"cell_type":"markdown","source":"## 2. Environment Setup and Library Imports","metadata":{}},{"cell_type":"code","source":"!pip install tqdm textstat langdetect lightgbm scikit-learn transformers sentence-transformers accelerate wordcloud -q\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport gc\nimport warnings\nimport re\nimport unicodedata\nfrom tqdm.auto import tqdm\nimport textstat\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom langdetect import detect, DetectorFactory, LangDetectException\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch.cuda.amp as amp\nfrom sentence_transformers import SentenceTransformer, util\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 150\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    nltk.download('punkt', quiet=True)\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept nltk.downloader.DownloadError:\n    nltk.download('stopwords', quiet=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:35:07.881869Z","iopub.execute_input":"2025-08-23T02:35:07.882214Z","iopub.status.idle":"2025-08-23T02:37:07.052137Z","shell.execute_reply.started":"2025-08-23T02:35:07.882185Z","shell.execute_reply":"2025-08-23T02:37:07.05127Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Configuration parameters","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    n_folds = 5\n    target_col = 'label'\n    data_path = '/kaggle/input/fake-or-real-the-impostor-hunt/data/'\n    train_path = os.path.join(data_path, 'train')\n    test_path = os.path.join(data_path, 'test')\n    train_csv_path = os.path.join(data_path, 'train.csv')\n    output_path = './'\n    model_name = 'microsoft/deberta-v3-base'\n    max_length = 512\n    batch_size = 4\n    n_epochs = 5\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    lgb_params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n        'n_estimators': 2000,\n        'learning_rate': 0.01,\n        'num_leaves': 31,\n        'max_depth': 7,\n        'seed': seed,\n        'n_jobs': -1,\n        'verbose': -1,\n        'device': 'gpu',\n        'colsample_bytree': 0.7,\n        'subsample': 0.7,\n    }\n    cascaded_confidence_threshold = 0.45\n\ndef seed_everything(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    DetectorFactory.seed = seed\n\nseed_everything(CFG.seed)\nstop_words = set(stopwords.words(\"english\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:37:07.053487Z","iopub.execute_input":"2025-08-23T02:37:07.054115Z","iopub.status.idle":"2025-08-23T02:37:07.067341Z","shell.execute_reply.started":"2025-08-23T02:37:07.054091Z","shell.execute_reply":"2025-08-23T02:37:07.066552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Data loading, preprocessing, and enhancement","metadata":{}},{"cell_type":"code","source":"def read_text_files(df, base_path):\n    texts_1, texts_2 = [], []\n    all_dirs = glob.glob(os.path.join(base_path, 'article_*'))\n    dir_map = {int(os.path.basename(p).replace('article_', '')): p for p in all_dirs}\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Reading files from {os.path.basename(base_path)}\"):\n        article_id = row['id']\n        dir_path = dir_map.get(article_id)\n        text1_content, text2_content = \"\", \"\"\n        if dir_path:\n            try:\n                with open(os.path.join(dir_path, 'file_1.txt'), 'r', encoding='utf-8') as f:\n                    text1_content = f.read()\n            except FileNotFoundError:\n                pass\n            try:\n                with open(os.path.join(dir_path, 'file_2.txt'), 'r', encoding='utf-8') as f:\n                    text2_content = f.read()\n            except FileNotFoundError:\n                pass\n        texts_1.append(text1_content)\n        texts_2.append(text2_content)\n    df['text_1'] = texts_1\n    df['text_2'] = texts_2\n    return df\n\ndef load_data(config):\n    train_df = pd.read_csv(config.train_csv_path)\n    train_df = read_text_files(train_df, config.train_path)\n    test_dirs = glob.glob(os.path.join(config.test_path, 'article_*'))\n    test_ids = [int(os.path.basename(p).replace('article_', '')) for p in test_dirs]\n    test_df = pd.DataFrame(sorted(test_ids), columns=['id'])\n    test_df = read_text_files(test_df, config.test_path)\n    train_df[config.target_col] = train_df['real_text_id'].apply(lambda x: 0 if x == 1 else 1)\n    print(f\"原始训练数据: {train_df.shape}\")\n    print(f\"测试数据: {test_df.shape}\")\n    return train_df, test_df\n\ntrain_df, test_df = load_data(CFG)\n\nprint(\"进行数据增强...\")\ndf_swap = train_df.copy()\ndf_swap['text_1'], df_swap['text_2'] = df_swap['text_2'], df_swap['text_1']\ndf_swap['label'] = 1 - df_swap['label']\ntrain_df_augmented = pd.concat((train_df, df_swap), axis=0).reset_index(drop=True)\nprint(f\"增强后训练数据: {train_df_augmented.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:37:07.068197Z","iopub.execute_input":"2025-08-23T02:37:07.068471Z","iopub.status.idle":"2025-08-23T02:37:17.966884Z","shell.execute_reply.started":"2025-08-23T02:37:07.068447Z","shell.execute_reply":"2025-08-23T02:37:17.966138Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. feature engineering（LightGBM）","metadata":{}},{"cell_type":"code","source":"def generate_stylometric_features(text):\n    if not isinstance(text, str) or not text.strip():\n        return {k: 0 for k in ['char_count', 'word_count', 'sentence_count', 'avg_word_length', 'flesch_reading_ease', 'gunning_fog', 'latin_ratio']}\n    text = unicodedata.normalize('NFC', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    words = word_tokenize(text)\n    sentences = sent_tokenize(text)\n    word_count = len(words)\n    if word_count == 0: return {k: 0 for k in ['char_count', 'word_count', 'sentence_count', 'avg_word_length', 'flesch_reading_ease', 'gunning_fog', 'latin_ratio']}\n    features = {}\n    features['char_count'] = len(text)\n    features['word_count'] = word_count\n    features['sentence_count'] = len(sentences)\n    features['avg_word_length'] = np.mean([len(w) for w in words])\n    try: features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n    except: features['flesch_reading_ease'] = 0\n    try: features['gunning_fog'] = textstat.gunning_fog(text)\n    except: features['gunning_fog'] = 0\n    non_space_chars = [c for c in text if c != ' ']\n    if non_space_chars:\n        latin_chars = [c for c in non_space_chars if 'LATIN' in unicodedata.name(c, '')]\n        features['latin_ratio'] = len(latin_chars) / len(non_space_chars)\n    else:\n        features['latin_ratio'] = 0\n    return features\n\nsbert_model = None\ntry:\n    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n    print(\"SentenceTransformer ('all-MiniLM-L6-v2') 加载成功.\")\nexcept Exception as e:\n    print(f\"无法加载 SentenceTransformer: {e}。语义特征将被跳过。\")\n\ndef create_differential_features(df):\n    print(\"开始提取文体学特征...\")\n    features_1 = df['text_1'].apply(generate_stylometric_features).apply(pd.Series)\n    features_2 = df['text_2'].apply(generate_stylometric_features).apply(pd.Series)\n    feature_cols = list(features_1.columns)\n    for col in tqdm(feature_cols, desc=\"创建差分特征(diff & ratio)\"):\n        df[f'{col}_diff'] = features_1[col].astype(float) - features_2[col].astype(float)\n        df[f'{col}_ratio'] = features_1[col].astype(float) / (features_2[col].astype(float) + 1e-9)\n    if sbert_model is not None:\n        print(\"开始计算语义特征...\")\n        embeddings1 = sbert_model.encode(df['text_1'].tolist(), show_progress_bar=True, batch_size=16)\n        embeddings2 = sbert_model.encode(df['text_2'].tolist(), show_progress_bar=True, batch_size=16)\n        df['cosine_similarity'] = [cosine_similarity([e1], [e2])[0][0] for e1, e2 in zip(embeddings1, embeddings2)]\n        df['euclidean_distance'] = [np.linalg.norm(e1 - e2) for e1, e2 in zip(embeddings1, embeddings2)]\n        print(\"语义特征计算完成。\")\n    final_feature_cols = [f'{col}_diff' for col in feature_cols] + \\\n                         [f'{col}_ratio' for col in feature_cols]\n    if 'cosine_similarity' in df.columns:\n        final_feature_cols.extend(['cosine_similarity', 'euclidean_distance'])\n    for col in final_feature_cols:\n        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).replace([np.inf, -np.inf], 0)\n    return df, final_feature_cols\n\ntrain_df_features, feature_cols = create_differential_features(train_df_augmented.copy())\ntest_df_features, _ = create_differential_features(test_df.copy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:37:17.967708Z","iopub.execute_input":"2025-08-23T02:37:17.967961Z","iopub.status.idle":"2025-08-23T02:37:45.691298Z","shell.execute_reply.started":"2025-08-23T02:37:17.967944Z","shell.execute_reply":"2025-08-23T02:37:45.690509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. deep learning model（DeBERTa）","metadata":{}},{"cell_type":"code","source":"class SiameseNetwork(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.backbone = AutoModel.from_pretrained(model_name)\n        hidden_size = self.backbone.config.hidden_size\n        self.interaction_head = nn.Sequential(\n            nn.Linear(hidden_size * 4, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, 1)\n        )\n    def forward_one(self, input_ids, attention_mask):\n        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.last_hidden_state[:, 0, :] # [CLS] token\n    def forward(self, input_ids_A, attention_mask_A, input_ids_B, attention_mask_B, labels=None):\n        vec_A = self.forward_one(input_ids_A, attention_mask_A)\n        vec_B = self.forward_one(input_ids_B, attention_mask_B)\n        diff = vec_A - vec_B\n        prod = vec_A * vec_B\n        combined_vec = torch.cat((vec_A, vec_B, diff, prod), dim=1)\n        logits = self.interaction_head(combined_vec)\n        loss = None\n        if labels is not None:\n            loss = nn.BCEWithLogitsLoss()(logits.view(-1), labels.float())\n        return (loss, logits)\n\nclass TextPairDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.texts1 = df['text_1'].values\n        self.texts2 = df['text_2'].values\n        self.labels = df[CFG.target_col].values if CFG.target_col in df.columns else None\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        encoding_A = self.tokenizer(self.texts1[idx], add_special_tokens=True, truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n        encoding_B = self.tokenizer(self.texts2[idx], add_special_tokens=True, truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n        item = {'input_ids_A': encoding_A['input_ids'].flatten(), 'attention_mask_A': encoding_A['attention_mask'].flatten(), 'input_ids_B': encoding_B['input_ids'].flatten(), 'attention_mask_B': encoding_B['attention_mask'].flatten()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:37:45.693421Z","iopub.execute_input":"2025-08-23T02:37:45.693956Z","iopub.status.idle":"2025-08-23T02:37:45.704287Z","shell.execute_reply.started":"2025-08-23T02:37:45.693935Z","shell.execute_reply":"2025-08-23T02:37:45.703335Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Cross-validation and model training","metadata":{}},{"cell_type":"code","source":"def train_eval_loop():\n    oof_lgbm = np.zeros(len(train_df_augmented))\n    test_preds_lgbm = np.zeros(len(test_df))\n    oof_deberta = np.zeros(len(train_df_augmented))\n    test_preds_deberta = np.zeros(len(test_df))\n\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n\n    skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df_augmented, train_df_augmented[CFG.target_col])):\n        print(f\"===== FOLD {fold+1}/{CFG.n_folds} =====\")\n        train_fold_df = train_df_features.iloc[train_idx]\n        val_fold_df = train_df_features.iloc[val_idx]\n\n        # --- LightGBM ---\n        print(\"Training LightGBM...\")\n        lgb_model = lgb.LGBMClassifier(**CFG.lgb_params)\n        lgb_model.fit(train_fold_df[feature_cols], train_fold_df[CFG.target_col],\n                      eval_set=[(val_fold_df[feature_cols], val_fold_df[CFG.target_col])],\n                      callbacks=[lgb.early_stopping(100, verbose=False)])\n        oof_lgbm[val_idx] = lgb_model.predict_proba(val_fold_df[feature_cols])[:, 1]\n        test_preds_lgbm += lgb_model.predict_proba(test_df_features[feature_cols])[:, 1] / CFG.n_folds\n\n        # --- DeBERTa ---\n        print(\"Training DeBERTa...\")\n        train_dataset = TextPairDataset(train_fold_df, tokenizer, CFG.max_length)\n        val_dataset = TextPairDataset(val_fold_df, tokenizer, CFG.max_length)\n        train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n        \n        model = SiameseNetwork(CFG.model_name).to(CFG.device)\n        optimizer = AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * CFG.n_epochs)\n        scaler = amp.GradScaler()\n        \n        for epoch in range(CFG.n_epochs):\n            model.train()\n            for batch in train_loader:\n                optimizer.zero_grad()\n                with amp.autocast():\n                    loss, _ = model(**{k: v.to(CFG.device) for k, v in batch.items()})\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n\n        model.eval()\n        fold_preds = []\n        with torch.no_grad():\n            for batch in val_loader:\n                 _, logits = model(**{k: v.to(CFG.device) for k, v in batch.items() if k != 'labels'})\n                 fold_preds.extend(torch.sigmoid(logits).cpu().numpy().flatten())\n        oof_deberta[val_idx] = fold_preds\n\n        test_dataset_full = TextPairDataset(test_df, tokenizer, CFG.max_length)\n        test_loader_full = DataLoader(test_dataset_full, batch_size=CFG.batch_size, shuffle=False)\n        test_fold_preds = []\n        with torch.no_grad():\n            for batch in test_loader_full:\n                _, logits = model(**{k: v.to(CFG.device) for k, v in batch.items() if k != 'labels'})\n                test_fold_preds.extend(torch.sigmoid(logits).cpu().numpy().flatten())\n        test_preds_deberta += np.array(test_fold_preds) / CFG.n_folds\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return oof_lgbm, test_preds_lgbm, oof_deberta, test_preds_deberta\n\noof_lgbm, test_lgbm, oof_deberta, test_deberta = train_eval_loop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:37:45.705164Z","iopub.execute_input":"2025-08-23T02:37:45.705457Z","iopub.status.idle":"2025-08-23T02:53:34.369389Z","shell.execute_reply.started":"2025-08-23T02:37:45.70543Z","shell.execute_reply":"2025-08-23T02:53:34.368677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"LGBM OOF Accuracy: {accuracy_score(train_df_augmented[CFG.target_col], np.round(oof_lgbm)):.5f}\")\nprint(f\"DeBERTa OOF Accuracy: {accuracy_score(train_df_augmented[CFG.target_col], np.round(oof_deberta)):.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T03:14:23.124393Z","iopub.execute_input":"2025-08-23T03:14:23.124743Z","iopub.status.idle":"2025-08-23T03:14:23.13338Z","shell.execute_reply.started":"2025-08-23T03:14:23.124722Z","shell.execute_reply":"2025-08-23T03:14:23.132584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import confusion_matrix\n# cm = confusion_matrix(train_df_augmented[CFG.target_col], np.round(oof_lgbm))\n# print(\"LGBM:\")\n# print(cm)  #0.48  0.95789  #0.47 0.96316 # 0.46 0.96842 \n# cm = confusion_matrix(train_df_augmented[CFG.target_col], oof_deberta>0.46)\n# print(f\"DeBERTa OOF Accuracy: {accuracy_score(train_df_augmented[CFG.target_col], oof_deberta>0.46):.5f}\")\n# print(\"DeBERTa:\")\n# print(cm)\n# cm = confusion_matrix(train_df_augmented[CFG.target_col], np.round(oof_deberta))\n# print(\"DeBERTa-0.5:\")\n# print(cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T03:18:55.848933Z","iopub.execute_input":"2025-08-23T03:18:55.849735Z","iopub.status.idle":"2025-08-23T03:18:55.863568Z","shell.execute_reply.started":"2025-08-23T03:18:55.849711Z","shell.execute_reply":"2025-08-23T03:18:55.862762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Model integration and submission","metadata":{}},{"cell_type":"markdown","source":"lightGBM Contribution to the test set needs to be considered","metadata":{}},{"cell_type":"code","source":"# # Blending\n# oof_blend = 0.5 * oof_lgbm + 0.5 * oof_deberta\n# test_blend = 0.5 * test_lgbm + 0.5 * test_deberta\n# print(f\"Blend OOF Accuracy: {accuracy_score(train_df_augmented[CFG.target_col], np.round(oof_blend)):.5f}\")\n\n# # Stacking\n# meta_X_train = np.column_stack([oof_lgbm, oof_deberta])\n# meta_X_test = np.column_stack([test_lgbm, test_deberta])\n# # meta_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', n_estimators=500, learning_rate=0.05, num_leaves=15, random_state=CFG.seed, verbose=-1)\n# # meta_model.fit(meta_X_train, train_df_augmented[CFG.target_col])\n# # final_preds_proba = meta_model.predict_proba(meta_X_test)[:, 1]\n\n# # Logistics\n# from sklearn.linear_model import LogisticRegression\n# meta_model = LogisticRegression(random_state=CFG.seed)\n# meta_model.fit(meta_X_train, train_df_augmented[CFG.target_col])\n# final_preds_proba = meta_model.predict_proba(meta_X_test)[:, 1]\n\n# # Submission\n# final_preds_class = (final_preds_proba > 0.5).astype(int)\n# submission_preds = [1 if pred == 0 else 2 for pred in final_preds_class]\n# submission_df = pd.DataFrame({'id': test_df['id'], 'real_text_id': submission_preds})\n# submission_df.to_csv('submission.csv', index=False)\n# print(\"Submission file 'submission.csv' created successfully!\")\n# print(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:53:34.384419Z","iopub.execute_input":"2025-08-23T02:53:34.385212Z","iopub.status.idle":"2025-08-23T02:53:34.441149Z","shell.execute_reply.started":"2025-08-23T02:53:34.385188Z","shell.execute_reply":"2025-08-23T02:53:34.43982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Blending\n# oof_blend = 0.5 * oof_lgbm + 0.5 * oof_deberta\n# test_blend = 0.5 * test_lgbm + 0.5 * test_deberta\n# print(f\"Blend OOF Accuracy: {accuracy_score(train_df_augmented[CFG.target_col], np.round(oof_blend)):.5f}\")\n\n# # Stacking\n# meta_X_train = np.column_stack([oof_lgbm, oof_deberta])\n# meta_X_test = np.column_stack([test_lgbm, test_deberta])\n# # meta_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', n_estimators=500, learning_rate=0.05, num_leaves=15, random_state=CFG.seed, verbose=-1)\n# # meta_model.fit(meta_X_train, train_df_augmented[CFG.target_col])\n# # final_preds_proba = meta_model.predict_proba(meta_X_test)[:, 1]\n\n# # Logistics\n# from sklearn.linear_model import LogisticRegression\n# meta_model = LogisticRegression(random_state=CFG.seed)\n# meta_model.fit(meta_X_train, train_df_augmented[CFG.target_col])\n# final_preds_proba = meta_model.predict_proba(meta_X_test)[:, 1]\n\n# Submission\nfinal_preds_class = (test_deberta > 0.5).astype(int)\nsubmission_preds = [1 if pred == 0 else 2 for pred in final_preds_class]\nsubmission_df = pd.DataFrame({'id': test_df['id'], 'real_text_id': submission_preds})\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' created successfully!\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T03:19:30.936586Z","iopub.execute_input":"2025-08-23T03:19:30.937314Z","iopub.status.idle":"2025-08-23T03:19:30.948212Z","shell.execute_reply.started":"2025-08-23T03:19:30.937285Z","shell.execute_reply":"2025-08-23T03:19:30.947341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Visual analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(oof_lgbm, bins=30, kde=True, color='skyblue')\nplt.title(\"LGBM OOF Probabilities\")\nplt.subplot(1, 2, 2)\nsns.histplot(oof_deberta, bins=30, kde=True, color='lightcoral')\nplt.title(\"DeBERTa OOF Probabilities\")\nplt.tight_layout()\nplt.show()\n\n# 全量数据训练一个LGBM以获取特征重要性\nfull_lgbm_model = lgb.LGBMClassifier(**CFG.lgb_params)\nfull_lgbm_model.fit(train_df_features[feature_cols], train_df_features[CFG.target_col])\nfeature_importance = pd.DataFrame({'feature': feature_cols, 'importance': full_lgbm_model.feature_importances_}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\nsns.barplot(x='importance', y='feature', data=feature_importance.head(20), palette=\"viridis\")\nplt.title(\"Top 20 LightGBM Feature Importance\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T02:53:34.450302Z","iopub.execute_input":"2025-08-23T02:53:34.450572Z","iopub.status.idle":"2025-08-23T02:53:41.808325Z","shell.execute_reply.started":"2025-08-23T02:53:34.450548Z","shell.execute_reply":"2025-08-23T02:53:41.807478Z"}},"outputs":[],"execution_count":null}]}